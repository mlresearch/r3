---
title: Why Averaging Classifiers can Protect Against Overfitting
url: http://www.gatsby.ucl.ac.uk/aistats/aistats2001/files/freund154.ps
timestamp: Wed, 06 May 2015 01:00:00 +0200
biburl: https://dblp.org/rec/conf/aistats/FreundMS01.bib
abstract: We study a simple learning algorithm for binary classification. Instead
  of predicting with the best hypothesis in the hypothesis class, this algorithm predicts
  with a weighted average of all hypotheses, weighted exponentially with respect to
  their training error. We show that the prediction of this algorithm is much more
  stable than the prediction of an algorithm that predicts with the best hypothesis.
  By allowing the algorithm to abstain from predicting on some examples, we show that
  the predictions it makes when it does not abstain are very reliable. Finally, we
  show that the probability that the algorithm abstains is comparable to the generalization
  error of the best hypothesis in the class.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: freund01a
month: 0
tex_title: Why Averaging Classifiers can Protect Against Overfitting
firstpage: 98
lastpage: 105
page: 98-105
order: 98
cycles: false
bibtex_editor: Richardson, Thomas S. and Jaakkola, Tommi S.
editor:
- given: Thomas S.
  family: Richardson
- given: Tommi S.
  family: Jaakkola
bibtex_author: Freund, Yoav and Mansour, Yishay and Schapire, Robert E.
author:
- given: Yoav
  family: Freund
- given: Yishay
  family: Mansour
- given: Robert E.
  family: Schapire
date: 2001-01-04
note: Reissued by PMLR on 31 March 2021.
address:
container-title: Proceedings of the Eighth International Workshop on Artificial Intelligence
  and Statistics
volume: R3
genre: inproceedings
issued:
  date-parts:
  - 2001
  - 1
  - 4
pdf: http://proceedings.mlr.press/r3/freund01a/freund01a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
